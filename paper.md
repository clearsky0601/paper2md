# MetaVers: Meta-Learned Versatile Representations for Personalized Federated Learning

## Abstract

Abstract:One of the daunting challenges in federated learning (FL) is the heterogeneity across clients that hinders the successful federation of a global model. When the heterogeneity becomes worse, personalized federated learning (PFL) pursues to detour the hardship of capturing the commonality across clients by allowing the personalization of models built upon the federation. In the scope of PFL for visual models, on the contrary, the recent effort for aggregating an effective global representation rather than chasing further personalization draws great attention. Along the same lines, we aim to train a large-margin global representation with a strong generalization across clients by adopting the meta-learning framework and margin-based loss, which are widely accepted to be effective in handling multiple visual tasks. Our method called MetaVers achieves state-of-the-art accuracies for the PFL benchmarks with the CIFAR-10, CIFAR-100, and CINIC-10 datasets while showing robustness against data reconstruction attacks. Noteworthy, the versatile representation of MetaVers exhibits a strong generalization when tested on new clients with novel classes. Code is available at https://github.com/eepLearning/MetaVers.


## Introduction

Recently, a massive number of data samples are being created at distributed devices such as wireless smart devices, connected self-driving cars, and other edge nodes. Collecting private data samples from these decentralized nodes to a centralized server for training models raises serious concerns about data privacy. As one of the ways to resolve the privacy concern, federated learning (FL) has been proposed to train a single yet effective global model by aggregating local models from clients [26] while keeping data on the client side.

However, such advances in decentralized learning framework may fail to acquire a generalized model when the data distribution across clients is widely diverse, so-called heterogeneity across clients. To relieve the challenge, personalized federated learning (PFL) aims to train a personalized model for each client by leveraging the benefits from the federation across clients rather than pursuing a single global model.

A group of PFL methods is based on regularizing the gap between personalized models and the global model. For instance, pFedMe [9] regularizes the gap in parameters between personalized and the global model, and FedProto [37] focuses on the representation space by regularizing the locally computed per-class averaged features not to diverge far from the globally computed per-class averaged features.

Another branch of methods aims to devise model-based approaches. FedPer [2] and LG-FedAvg [23] , explicitly divide learnable parameters into local and global, i.e., the local parameters are optimized at each client, but the global parameters are trained via federation across clients. Other methods called pFedHN of [34] and pFedGP of [1] introduce auxiliary networks that are trained over clients which facilitate the construction of client-specific local models. Most of the prior PFL algorithms focus on acquiring sufficient personalization of local models based on the assistance of the federation that leverages benefits across clients. It has been believed that the training of a global model is sub-optimal for handling heterogeneity across clients.

A few recent works [6] , [25] , [28] argue the importance of the successful aggregation of a global model that overcomes heterogeneity across clients. FedRep [6] and FedBABU [28] try to learn a shared representation across clients by decomposing the training of the feature extractor and classifiers, i.e., federation takes place only for the extractor, and the classifier is personalized for each client. In the work of FedRep, authors first claim that the global representation fully leverages the commonality across clients and can be easily generalized to new clients with novel classes. FedBABU [28] reveals that the federation of the extractor so-called ‘body’ is the key to federated learning of deep models across clients. Once the body is trained across clients, further steps of personalization on each client with a personalized classifier achieve a noticeable performance than a naive case where the federation takes place for a whole model. Along the same lines, kNN-Per of [25] adopts k -nearest neighbors as the personalized classifiers while a global extractor is learned across clients. The stream of approaches claims the importance of a global representation that learns the way of the common feature extraction of the heterogeneous regime. However, these works only attempt to decouple the training of representation and classifiers for federated learning but do not employ particular learning strategies that are known to be effective for capturing the commonality across heterogeneity.

In this paper, a method called MetaVers exploits two powerful methodologies to achieve a versatile representation via federation: meta-learning and margin-based learning, which are confirmed to be effective for acquiring strong generalization for multi-task settings and visual representation learning, respectively. To tackle the heterogeneity across clients, MetaVers borrows the concept of distance-based meta-learning such as ProtoNet of [36] , which aims to train a common feature extractor for different classification tasks. Each local few-shot episode for each client plays a distinctive classification task so that the aggregation of local gradients across clients leads to the meta-learning of the representation. Along with the meta-training framework, MetaVers encourages the large margin representation via adopting centroid triplet loss of [15] in computing local gradients. To prevent dense representation from local episodic training of scarce data, a server encourages all clients to learn sufficiently large-margin embeddings by sending the increasing margin values for the centroid triplet loss term that enables dynamic margins of the representation space.

In the extensive simulations, MetaVers achieves stateof-the-art performance on the standard PFL benchmarks based on the CIFAR-10, CIFAR-100, and CINIC-10 datasets. Also, we demonstrate the versatility of MetaVers to classify out-of-distribution data samples by measuring the performance of a newcomer client with novel classes that have not been trained at all. In the privacy perspective, MetaVers shows the robustness under the popular model inversion attack such as Deep Leakage from Gradients (DLG) [43] .


## Related Work

After the work of [26] called FedAvg , which is widely accepted as a baseline of federated learning (FL), immense efforts have been dedicated to tackling the heterogeneity across clients. We categorize related works into three parts: i) FL with heterogeneous clients whose goal is to achieve higher accuracy on an overall test split with a single global model, ii) Personalized federated learning (PFL) that pursues to improve the local performance of each client and iii) Distance-based meta-learning.

The heterogeneity across clients, also known as the non-independent and identically distributed (non-IID) setting, is shown to hinder the aggregated model from converging to the optimal global model [18] , [22] , [32] . When the heterogeneity becomes severe, locally trained model parameters of different clients largely diverge from each other so that the accuracy of the FL dramatically deterioates [42] . To solve this problem, various types of approaches have been proposed by extending FedAvg baseline [ 16 , 21 , 27 , 30 , 32 , 39 , 42 ]. The work of [42] claims that a small set of shared data is sufficient to prevent the divergence of local models. FedAvgM [16] considers the momentum of gradients to regularize the dramatic change in the global model. FedMA [39] aggregates a selected part of parameters, i.e., layer-wise federation inspired by Probabilistic Federated Neural Matching [41] . These early works for handling non-IID settings rather focus on regularizing the local training to guarantee the convergence of the global model. In contrast, MetaVers explicitly adopts meta-learning that enables training a common model rather than regularizing the diversity of local training across clients. FedAwS [40] aggregates both the model parameters and the class embeddings. This method utilizes the positive term of the contrastive loss to update the local model. However, sharing the class embeddings, which can contain a private data, contravenes a privacy perspective of FL scenario.

PFL allows each client to prepare its own personalized model by taking the benefits from federation across clients.

A branch of methods employs the auxiliary networks, which facilitate the personalization of each client’s model. pFedHN [34] learns a globally federated hypernetwork that aims to generate a personalized model for each client. Another method called pFedGP [1] combines Gaussian processes with the PFL framework to achieve an effective deep kernel function across all clients.

Another group of approaches tries to split models into parts and handle them separately in federation. Some prior works [2] , [8] , [13] , [23] combine a certain part of the aggregated model with local models. Other works divide the model architecture into local and global training parts: FedPer [2] locally trains a personal classifier layer with a globally trained feature extractor. In contrast, LG-FedAvg [23] locally trains feature extractors with a global classifier for reduced communication burden.

A method called FedProto [37] aggregates per-class averaged features, i.e., prototype, instead of gradients. In this setting, each client trains its personal model by utilizing the aggregated global prototype to regularize local prototypes closer to the global one.

When interpreting the training of each local client as ‘task’, then the learning strategies to handle multiple tasks can be borrowed to improve PFL methods. For instance, MOCHA [35] and FedU [10] adopt multi-task learning while training distinct but similar personal models. Another group of methods interprets PFL as transfer learning, so they aim to transfer knowledge from the global model to local models [5] , [20] . By expanding the multi-task learning viewpoint to meta-learning which pursues to extract the generalized knowledge from the given task distribution, the optimization-based meta-learning is utilized to tackle PFL [4] , [11] , [17] . Among them, Per-FedAvg [11] understands the objective of PFL as closely related to the bilevel optimization setting of Model-Agnostic Meta-Learning (MAML) of [12] . Our MetaVers and Per-FedAvg are relevant in view of meta-learning, but they show significant differences in both technical and philosophical viewpoints.

In very recent works of [6] , [25] , [28] , researchers emphasize the importance of the shared representation in the following perspectives: i) The successful federation of a global model over heterogeneity fully captures the commonality across clients [6] . ii) The global representation enables rapid personalization via a small number of updates [28] . iii) Moreover, the representation can work well even for a newcomer client who has never participated in the federation [6] , [25] . Their strategies are based on decoupling the training of the representation part and the classifier part, which are called ‘body’ and ‘head’, respectively. FedRep of [6] freezes locally-trained classifiers while training the feature extractor, then the parameters of the extractor are aggregated across clients. FedBABU of [28] never learns the classifiers but only trains the feature extractor during the federation. kNN-Per of [25] adopts k -nearest neighbors as the classifier and trains the global feature extractor across clients.

Comparison to prototype-based method: In the view of utilizing prototypes, FedProto [37] seems to be closely related to our work, but there is a significant difference, i.e., FedProto does not aggregate model parameters but collects the global prototypes as the tool for regularizing local training so that FedProto cannot obtain a meta-trained model.

Per-FedAvg [11] is built upon the concept of optimization-based meta-learning which requires fine-tuning steps. Due to the nature of optimization-based approaches, Per-FedAvg requires further fine-tuning of the global model to work successfully on the client side. When comparing with our MetaVers , the global model of Per-FedAvg cannot work as a generalized representation because it relies on local fine-tuning. In contrast, MetaVers is based on distance-based meta-learning such as Prototypical Networks of [36] , which focuses on the training of the global feature extractor so that the shared embedding of MetaVers is capable of acquiring the sufficient personalization at each local client without an additional update.

As the prior works including FedRep [6] , kNN-Per [25] and FedBABU [28] , MetaVers never trains local classifiers but computes prototypes for each local episode as the classifiers so that the federation is focused on finding a global representation. The main difference is that MetaVers employs two explicit methodologies to fully aggregate the commonality across clients, i.e., the meta-learning framework and large-margin loss. As a result, MetaVers outperforms the existing methods in the PFL benchmarks and shows noticeable performance for a newcomer client who contains novel categories that have not been trained before. We emphasize that MetaVers offers the advanced method to train a strong global representation across clients.

Table 1 shows the comparison between MetaVers and the existing PFL methods from three different viewpoints: the federation of a global body, head classifier type, and fine-tuning steps for personalization in testing.

When saying the relevant method, Prototypical Networks [36] train a representation across widely-varying classification, where the features from the same class are concentrated to the class-specific averaged features so-called prototype. MetaVers extends the concept of Prototypical Networks into the PFL settings to train a representation across varying episodes from different clients. Each client of MetaVers processes its episodes that resemble Prototypical Networks, and the computed gradients are aggregated at the server.


## Proposed Method

In the PFL setting, the number of distributed clients is n. Each client contains the local dataset ${\mathcal{D}_i}$ , which follows the data distribution ${\mathcal{P}_i}$ . The embedding model for feature extraction is f ( • ; θ ) with model parameter θ , and the classifier weight is ϕi for client i. The local loss computed at i -th client is ℒ( f ( x ; θi ) ,ϕi,y ). Then the objective function of PFL is to find the client-specific model parameters $\left\{ {{\Theta ^{\ast}},{\Phi ^{\ast}}} \right\} = \left\{ {\theta _i^{\ast},\phi _i^{\ast}} \right\}_{i = 1}^n$ that minimizes averaged local loss values across clients. For the purpose of finding a shared representation across clients, the feature extractor weight should be a single global model, i.e., ${\theta ^ * } = {\theta _i}^ *$ for all i ∈ [ n ]. In addition, MetaVers does not require training classifier weights, so we can further drop the classifier weights in the objective function: ${\theta ^{\ast}} = {\operatorname{argmin} _\theta }\frac{1}{n}\sum\nolimits_{i = 1}^n {\frac{1}{{\left| {{\mathcal{D}_i}} \right|}}} \sum\nolimits_{(x,y)\backslash {\mathcal{P}_i}} \mathcal{L} (f(x;\theta ),y)$ .

Our proposed algorithm, MetaVers , handles repetitive communication rounds in the same way as FedAvg of [26] .

At the beginning of round τ = 1,••• ,T , a client receives a global embedding network f ( • ; θ (τ) ) parameterized by θ (τ) . Also, the central server transmits the round-specific distance margin value $\mathfrak{m}_{{\text{global}}}^{(\tau )}$ to every client.

In each round, each client constructs its own training episode $\mathcal{E}_i^{(\tau )}$ . The episode is generated by sampling N local classes and their labeled samples. First of all, the support set $\mathcal{S}$ with K samples per class and the query set $\mathcal{Q}$ with Q samples per class are sampled. The prototype c k for local class k is computed by taking the average of feature vectors from support samples: 
$$\begin{equation*}{{\mathbf{c}}_k} = \frac{1}{{\left| {{\mathcal{S}_k}} \right|}}\sum\limits_{x \in {\mathcal{S}_k}} f \left( {x;{\theta ^{(\tau )}}} \right),\tag{1}\end{equation*}$$
 where ${\mathcal{S}_k}$ is the subset of $\mathcal{S}$ whose label is k ∈ {1 ,...,N }.

Each client calculates the Euclidean distance-based cross-entropy loss ℒ S by measuring the distance between the feature vectors of query samples and the class `prototypes`, i.e., 
$$\begin{align*} {\mathcal{L}_S} = \frac{1}{{|\mathcal{Q}|}}\sum\limits_{k = 1}^N {\sum\limits_{x \in {\mathcal{Q}_k}} {\left[ {d\left( {f\left( {x;{\theta ^{(\tau )}}} \right),{{\mathbf{c}}_k}} \right)} \right.} }  \left. { + \log \sum\limits_{l = 1}^N {\exp } \left( { - d\left( {f\left( {x;{\theta ^{(\tau )}}} \right),{{\mathbf{c}}_l}} \right)} \right)} \right]. \tag{2}\end{align*}$$


To promote a well-clustered representation, MetaVers employs a particular aggregation of margins values. First, each client computes the class centroid a k which is the perclass averaged features of the samples in its local episode, including supports and queries. The average distance between different centroids, so-called local distance margin $\mathfrak{m}_i^{(\tau )}$ is then computed: 
$$\begin{equation*}\mathfrak{m}_i^{(\tau )} = \frac{1}{{{{(N - 1)}^2}}}\sum\limits_k {\sum\limits_{l \ne k} d } \left( {{{\mathbf{a}}_k},{{\mathbf{a}}_l}} \right)\tag{3}\end{equation*}$$


The client-specific margin value $\mathfrak{m}_i^{\ast}$ for the triplet loss is obtained by taking the larger value among the local and global margins, i.e., $\mathfrak{m}_i^{\ast} = \max \left\{ {\mathfrak{m}_{{\text{global }}}^{(\tau )},\mathfrak{m}_i^{(\tau )}} \right\}$ . For centroid triplet loss, the centroid a k is used as the anchor point. The positive points are sampled from the features of the support and query samples from class k. The negative points are samples from the feature vectors of supports and queries from other classes. Then for a triplet $\left( {{{\text{a}}_k},{x_p} \in {\mathcal{S}_k} \cup {\mathcal{Q}_k},{x_n} \in {\mathcal{S}_l} \cup {\mathcal{Q}_l}} \right)$ where l ≠ k , the loss can be calculated: 
$$\begin{align*} {\mathcal{L}_T}\left( {{{\mathbf{a}}_k},{x_p},{x_n}} \right) = \max \left\{ {d\left( {{{\mathbf{a}}_k},f\left( {{x_p};{\theta ^{(\tau )}}} \right)} \right)} \right.  \left. { - d\left( {f\left( {{x_p};{\theta ^{(\tau )}}} \right),f\left( {{x_n};{\theta ^{(\tau )}}} \right)} \right) + \mathfrak{m}_i^{\ast},0} \right\}. \tag{4}\end{align*}$$


By considering all cases, the triplet loss term ℒ T is obtained: ${\mathcal{L}_T} = \sum\nolimits_k {\sum\nolimits_{\left( {{x_p},{x_n}} \right)} {{\mathcal{L}_T}} } \left( {{{\mathbf{a}}_k},{x_p},{x_n}} \right)$ . The computational overhead of centroid triplet loss for MetaVers is: O ( cN 2 ), where c is the number of classes and N is the number of samples. It seems to be burdensome at a glance, because it is square of the number of the sample. The reason why we adopt triplet-based loss is that triplet loss is directly designed to enlarge the margin between classes in a sample-by-sample way. Also, using a single anchor point, which is the per-class centroid, reduces the number of pairs to be considered when compared with conventional triplet loss of [33] . Our triplet loss computation is similar to the triplet-center loss proposed in the work of [15] . The difference is that we consider entire feature vectors from different classes in the negative pair terms but the triplet-center loss of [15] utilizes different class centroids as negative points. That makes MetaVers enlarge the margin more strongly.

MetaVers makes each client adopt a larger margin than the local distance. The round and client-specific margin value guides each client to acquire better representation, i.e., when the local embedding is already well-separated and clustered than the global margin, then the client adopts its own local distance m i as the margin, otherwise, the client takes the global margin value ${\mathfrak{m}_{{\text{global}}}}$ from the server to promote the local embeddings to show the larger margin separation.

By combining cross-entropy loss ℒ S and triplet loss ℒ T , the embedding network f ( • ; θ (τ) ) is then updated: 
$$\begin{equation*}\theta _i^{(\tau )} \leftarrow {\theta ^{(\tau )}} - \eta \nabla \left( {\gamma {\mathcal{L}_S} + (1 - \gamma ){\mathcal{L}_T}} \right),\tag{5}\end{equation*}$$
 where γ is a hyperparameter for balancing the cross-entropy loss and the triplet loss. The locally updated parameter $\theta _i^{(\tau )}$ and the average distance between centroids $\mathfrak{m}_i^{(\tau )}$ are then uploaded to the server when client i is active in this round.

Federated learning process of MetaVers

Show All

The average of the aggregated local models from active clients is set to be the global model for the next round: 
$$\begin{equation*}{\theta ^{(\tau + 1)}} \leftarrow \frac{1}{{\left| {{C_\tau }} \right|}}\sum\limits_{i \in {C_\tau }} {\theta _i^{(\tau )}} ,\tag{6}\end{equation*}$$
 where Cτ is a set of active clients during round τ. The server aggregates the local average distance values $\mathfrak{m}_i^{(\tau )}$ from the active clients to newly calculate the average distance values across clients. For a stable update, the server considers the past global margin values with a fixed interval of W rounds: 
$$\begin{equation*}\mathfrak{m}_{{\text{global }}}^{(\tau + 1)} \leftarrow \frac{1}{W}\left\{ {\sum\limits_{t = \tau - W + 1}^{\tau - 1} {\mathfrak{m}_{{\text{global }}}^{(t)}} + \frac{1}{{\left| {{C_\tau }} \right|}}\sum\limits_{i \in {C_\tau }} {{\mathfrak{m}_i}} } \right\}.\tag{7}\end{equation*}$$


Figure 1 shows the learning process. Also, the pseudocode is in Supplementary material.

After T rounds, each client utilizes the shared representation f (•; θT ) as a feature extractor without further optimization. Each client then computes the per-class averaged features, i.e., prototypes of its local classes, and uses them as classifiers. Queries are classified into the nearest prototypes by computing the Euclidean distance metric.

Let us recall the triplet loss term of eq. (4) . Consider a triplet ( a k,xp,xn ) that produces non-zero triplet loss value. Without losing generality, let us assume that there are two different classes k and l in the given episode. Then the triplet loss term for the given triplet ( a k,xp,xn ) becomes 
$$\begin{align*} {\mathcal{L}_T} & = d\left( {{{\mathbf{a}}_k},f\left( {{x_p};\theta } \right)} \right) - d\left( {f\left( {{x_p};\theta } \right),f\left( {{x_n};\theta } \right)} \right) + \mathfrak{m}_i^{\ast} \\ & = \left\| {f\left( {{x_p};\theta } \right) - {{\mathbf{a}}_k}} \right\| - \left\| {f\left( {{x_p};\theta } \right) - f\left( {{x_n};\theta } \right)} \right\| + \mathfrak{m}_i^{\ast} \\ & \mathop \leq \limits^{(a)} \left\| {f\left( {{x_n};\theta } \right) - {{\mathbf{a}}_k}} \right\| + \mathfrak{m}_i^{\ast}. \tag{8}\end{align*}$$


The inequality (a) follows the fact that ||x − y|| − ||x −||z ≤ ||y − z||. When the global margin ${\mathfrak{m}_{{\text{global}}}}$ from the server is larger than the local average of the distance between centroids, i.e., ${\mathfrak{m}_{{\text{global }}}} > {\mathfrak{m}_i} = \left\| {{{\mathbf{a}}_k} - {{\mathbf{a}}_l}} \right\|$ , then the loss is upper bounded as follows: 
$$\begin{align*} {\mathcal{L}_T} & \leq \left\| {f\left( {{x_n};\theta } \right) - {{\mathbf{a}}_k}} \right\| + \left\| {{{\mathbf{a}}_k} - {{\mathbf{a}}_l}} \right\| + \Delta \\ & \leq \left\| {f\left( {{x_n};\theta } \right) - {{\mathbf{a}}_l}} \right\| + \Delta \tag{9}\end{align*}$$
 where $\Delta {\text{ }} = {\mathfrak{m}_{{\text{global}}}} - {\mathfrak{m}_i} > 0$ . To suppress the bound, the local margin m i should be enlarged to the global margin ${\mathfrak{m}_{{\text{global}}}}$ . Moreover, the distance between queries and the corresponding prototype should be reduced. It implies that the clients with less-separated class prototypes are encouraged to learn a better representation with a sufficient margin.

Herein, we provide a brief result of the convergence analysis. The full description including mathematical definitions, assumptions and proofs are in the Supplementary material.

${\mathcal{E}_i}$ represents a training episode at client . iθ (τ) indicates the global model parameter at the beginning of the round τ. $\theta _i^{(\tau )}$ means the locally updated model parameter after an episodic-training at client i in the round τ. Finally, ${\mathcal{L}_i}\left( {\theta ;{\mathcal{E}_i}} \right)$ is the local loss value based on the model parameter θ and the given episode ${\mathcal{E}_i}$ from client i. Also, L is the L-smoothness of local loss function. α ∈ (0,1] and σ 2 are used for bounding the local gradients. Based on the notation, following Lemma and Theorem are guaranteed.

Lemma 1. For every client i ∈ [1 ,n ] , the difference of local losses at round τ + 1 and τ is bounded:

where η is the learning rate of local update.

Theorem 1. (Convergence) For any client i ∈ [1 ,n ] with a learning rate ${\eta ^{\ast}} < \frac{{2\alpha }}{L}$ , the local loss is a decreasing function in the number of rounds:

The mathematical claim guarantees the decreasing behavior of the local loss function, which directly implies the convergence of the PFL performance of MetaVers .

The results with SEM (Standard Error of the Mean) are in Supplementary material.


## Experiments

MetaVers is evaluated on the various personalized federated learning (PFL) setups. Also, we run additional experiments to verify the strong generalization of the learned representation.

We compare MetaVers with other methods on the recent PFL settings that are considered in [1] , [34] . Among the existing PFL benchmarks, we select the setting in [1] , [34] due to the following two reasons: i) a wide range of the number of clients and ii) a strongly limited number of active clients. We believe that the settings can reflect realistic decentralized training setups. The benchmarks are based on the following datasets: CIFAR-10, CIFAR-100, and CINIC-10 [7] . CINIC-10 is a larger dataset that collects samples from two datasets: CIFAR-10 and ImageNet [31] . In our experiment, we set the total number of clients to be 50, 100, and 500. Also, the number of active clients for each round is 5 for all cases. We set the total number of classes in each client to be 2, 4, and 10 classes for CIFAR-10, CINIC-10, and CIFAR-100, respectively. Details for dataset splitting and the non-IID settings are described in Supplementary material. In a recent work of [3] , a PFL benchmark with CIFAR-10 is suggested by imposing heterogeneity through Dirichlet allocation. When compared to the Standard PFL benchmarks of [1] , [34] , less number of clients are given, i.e., 100 clients, and more active clients are allowed, i.e., 20 active clients. However, the degree of heterogeneity can be controlled by Dirichlet allocation. The evaluation on the benchmark is in Supplementary material.

By following the settings of [1] , [34] , we allow 1,000 server-client communication rounds. Five active clients are selected in each round besides pFedHN. For ‘Local’ method, each local model is allowed to be trained with 100 local episodes without communications. ‘LG-FedAvg’ requires extra 200 rounds after pretraining the FedAvg model via 1,000 rounds. 10 fine-tuning steps are used for FedBABU in testing.

As done in [1] , [34] , we use a LeNet-based model [19] with two 2 x 2 convolutional layers where a 2 x 2 max-pooling layer follows each one. After the second max-pooling layer, two fully-connected layers and one classifier layer follow. For MetaVers , the last classifier layer is not used. Details on the optimizer and learning rates are in Supplementary material. For demonstrating how the algorithm scales in the size of model architecture, we additionally demonstrate MetaVers with the ResNet architecture [14] in Supplementary material.

Table 2 shows the test accuracies averaged over three random seeds. We claim that i ) MetaVers achieves state-of-the-art performance with considerable margins for all cases. ii ) MetaVers are more solid in the case with more clients when compared to prior methods. We emphasize that the margins of accuracies between MetaVers and the runner-up algorithms are considerable when the number of clients increases, i.e., the gaps for 500-client cases are +2.3%, +5.2%, and +4.4% for CIFAR-10/100 and CINIC100, respectively. In these cases, each client contains a very small number of samples where the local training suffers from overfitting. MetaVers is robust for this scenario because each client newly constructs a few-shot episode by picking a very small number of samples for each round rather than fitting to its whole dataset so that it prevents overfitting of local updates. iii ) MetaVers achieves more dominant performance gains in more complicated datasets when compared to prior methods. MetaVers achieves outperforming performance in the CIFAR-100 cases with more diverse image categories, i.e., the gaps over the runner-ups are +3.4%, +3.5%, and +5.2% for 50, 100, and 500 client cases, respectively. This advantage is from the strength of meta-learning, which is more solid in training the shared knowledge of wide-range task distribution. iv ) When compared with FedProto of [37] , and Per-FedAvg of [11] that utilize prototype-based learning and optimization-based meta-learning, MetaVers shows outstanding performance. Also, we note that FedProto suffers from the limited number of active clients in our experiments, where the work of FedProto assumes full participation of clients at every round. v) For the prior methods with shared representations, including FedPer of [2] , FedRep of [6] , kNN-Per of [25] , and FedBABU of [28] , MetaVers is only the method that shows consistent gains for all benchmarks over the personal model-based PFL methods such as pFedHN of [34] and pFedGP of [1] . It confirms the superiority of the strong generalization capability of MetaVers to prior shared-representation-based approaches.

t-SNE for (a) Local (no federation) and MetaVers with (b) Only Triplet (w/ fixed margin), (c) Only Cross-Entropy and (d)

Show All

MetaVers does not have a learnable classifier, not only in inference but also in the training process. Instead, MetaVers computes local prototypes as the classifiers. We tested other methods with a shared representation, such as FedAvg, FedBABU, and FedRep, by changing their classifiers into prototype-based classifiers as MetaVers . We conjecture that the prototype-based classification reflects how much the global representation is intra-class compact and inter-class separated. Also, pFedGP is included as a cutting-edge PFL algorithm with personal models. As shown in Table 3 , MetaVers shows the best performance implying that the representation capability of MetaVers is outperforming.

To quantify how much the global representation shows well-clustered feature distribution, we computed normalized Variance (nVAR), which is the mean of per-class variance of features divided by the squared distance to the nearest interfering prototype: ${\text{nVAR}} = {\mathbb{E}_{k \in \mathcal{C}}}\left[ {{\mathbb{E}_{({\mathbf{x}},y) \in \mathcal{D}}}\left[ {{{\left\| {{{\mathbf{c}}_k} - f({\mathbf{x}};\theta )} \right\|}^2}/{{\left\| {{{\mathbf{c}}_k} - {{\mathbf{n}}_k}} \right\|}^2}} \right]} \right]$ , where $\mathcal{C}$ is the set of classes, i.e, 100 for the CIFAR-100 case, $\mathcal{D}$ is the union of test samples across clients, k is the class index, c k is the prototype of class k , f (•; θ ) is the feature extractor of the global representation for each algorithm, and n k is the nearest interfering prototype of class k. A smaller nVAR value indicates that the feature distributions are well-clustered and separated, which means a strong generalization over clients. The results in Table 4 clearly show that MetaVers acquires a compact and separated representation compared to other shared methods with a global representation.

We visualize the learned representation of MetaVers by using t-SNE of [38] to confirm the large-margin representation learning of MetaVers . As shown in Figure 2 , we compare (d) MetaVers with (a) Local, b) MetaVers with a fixed-margin triplet loss, and (c) MetaVers with the cross-entropy loss. In Figure 2 , the small dots in different colors indicate samples in 10 different classes of CINIC-10, and large points represent the class centroids. We observe that (d) MetaVers is the only case where the centroids are sufficiently well-separated without any overlapping. Notably, for cases (a), (b), and (c), some centroids are closely located to hinder the discrimination between classes. The result confirms that MetaVers dynamically enlarges the margins.

Single image reconstruction leakage after 300 iterations.

Show All

Accuracies for the variants of loss adaptation for MetaVers are shown in Table 5 . MetaVers with the dynamic margin shows the best performance over the versions of the fixed-margin ( m = 0.75) triplet loss and cross-entropy (CE) loss. Also, we emphasize that all the versions of MetaVers show a moderate decline in accuracies as the number of clients increases.

A recent approach called Deep Leakage from Gradients of [43] raises a crucial threat to the FL framework that aggregates the local gradients at the central server. DLG optimizes a dummy input to mimic shared local gradients, gradually approaching the original input sample, and repeatedly rehearses loss and gradient computations for data reconstruction. For MetaVers , however, a server cannot access the local class prototypes which are essential for rehearsing the loss computation. Therefore, MetaVers can be robust to leakage from gradients. To prove the concept, we actually carry out the single image reconstruction experiments for FedAvg [26] and MetaVers . For further analysis, we tested a variant of MetaVers that shares the local prototypes with the server (denoted as MetaVers with prototype leakage). In the experiments on CIFAR-10 and CIFAR-100, we perform 300 iterations for estimating the original image via the L-BFGS optimizers [24] with a learning rate of 1. As shown in Figure 3 , the attack on FedAvg and MetaVers with prototype leakage appears to be successful. Surprisingly, MetaVers is shown to prevent the attacker from reconstructing the images without any portion of data leakage. MetaVers does not share the prototypes so the estimation of local prototypes is essential for DLG to rehearse the loss computation, and it is shown to be a very challenging task for the attacker due to the large dimension of prototypes. Consequently, we confirm that MetaVers can train an effective representation without taking the risk of data leakage.

We evaluate MetaVers on the other protocol for unseen class inference from Few-Round Learning (FRL) of [29] . Following the exact setting of FRL, we partition CIFAR-100 and mini ImageNet into 64 train, 16 validation, and 20 test classes. Training is done for the train split with the nonIID setup, then the model is tested on a newcomer client with five unseen classes from the test split. We describe the exact settings in the Supplementary material. Although few-round learning (FRL) requires a few-round of additional communications for adapting the model to the novel client, MetaVers is tested without any further optimization of model parameters. In Table 6 , we can observe the strong generalization capability MetaVers on even novel clients.

Additional studies for the behavior of dynamic margin values are presented in the Supplementary material.


## Conclusions

We propose a meta-learning-based personalized federated learning (PFL) called MetaVers for versatile and largemargin representations. We adopt meta-learning for the PFL setting that aggregates the gradients from varying local episodes. In addition, a particular dynamic margin learning promotes better-clustered representations. MetaVers outperforms other competing methods in PFL benchmarks.

